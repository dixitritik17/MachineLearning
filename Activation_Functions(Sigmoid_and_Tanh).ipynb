{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation_Functions(Sigmoid_and_Tanh).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dixitritik17/MachineLearning/blob/master/Activation_Functions(Sigmoid_and_Tanh).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWyTLCF8IgZz",
        "colab_type": "code",
        "outputId": "d1224781-6344-4f23-fdf6-9ffc8f2aae3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''The training process consists of the following steps:\n",
        "\n",
        "Forward Propagation:\n",
        "Take the inputs, multiply by the weights (just use random numbers as weights)\n",
        "Let Y = Wi*Ii = W1*I1+W2*I2+W3*I3\n",
        "Pass the result through a sigmoid formula to calculate the neuron’s output. The Sigmoid function is used to normalise the result between 0 and 1:\n",
        "1/(1 + e^-y)\n",
        "\n",
        "\n",
        "Back Propagation\n",
        "\n",
        "Calculate the error i.e the difference between the actual output and the expected output. Depending on the error, adjust the weights by multiplying the error with the input and again with the gradient of the Sigmoid curve:\n",
        "Weight += Error *Input *derivative of sigmoid ,here Output (1-Output) is derivative of sigmoid curve.'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The training process consists of the following steps:\\n\\nForward Propagation:\\nTake the inputs, multiply by the weights (just use random numbers as weights)\\nLet Y = Wi*Ii = W1*I1+W2*I2+W3*I3\\nPass the result through a sigmoid formula to calculate the neuron’s output. The Sigmoid function is used to normalise the result between 0 and 1:\\n1/(1 + e^-y)\\n\\n\\nBack Propagation\\n\\nCalculate the error i.e the difference between the actual output and the expected output. Depending on the error, adjust the weights by multiplying the error with the input and again with the gradient of the Sigmoid curve:\\nWeight += Error *Input *derivative of sigmoid ,here Output (1-Output) is derivative of sigmoid curve.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsfjrvv0DqVS",
        "colab_type": "text"
      },
      "source": [
        "**Training Using Sigmoid as Activation Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SULlKnE0JByr",
        "colab_type": "code",
        "outputId": "53abd559-0997-406a-f1cd-d138be943d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import *\n",
        "  \n",
        "class NeuralNet(object): \n",
        "    def __init__(self): \n",
        "        # Generate random numbers \n",
        "        random.seed(1) \n",
        "  \n",
        "        # Assign random weights to a 3 x 1 matrix, \n",
        "        self.synaptic_weights = np.random.randn(3, 1)\n",
        "  \n",
        "    # The Sigmoid function \n",
        "    def __sigmoid(self, x): \n",
        "        return 1 / (1 + exp(-x)) \n",
        "  \n",
        "    # The derivative of the Sigmoid function. \n",
        "    # This is the gradient of the Sigmoid curve. \n",
        "    def __sigmoid_derivative(self, x): \n",
        "        return x * (1 - x) \n",
        "  \n",
        "    # Train the neural network and adjust the weights each time. \n",
        "    def train(self, inputs, outputs, training_iterations): \n",
        "        for iteration in range(training_iterations): \n",
        "  \n",
        "            # Pass the training set through the network. \n",
        "            output = self.learn(inputs) \n",
        "  \n",
        "            # Calculate the error \n",
        "            error = outputs - output \n",
        "  \n",
        "            # Adjust the weights by a factor \n",
        "            factor = dot(inputs.T, error * self.__sigmoid_derivative(output)) \n",
        "            self.synaptic_weights += factor \n",
        "  \n",
        "    # The neural network thinks. \n",
        "    def learn(self, inputs): \n",
        "        return self.__sigmoid(dot(inputs, self.synaptic_weights)) \n",
        "  \n",
        "if __name__ == \"__main__\": \n",
        "  \n",
        "    #Initialize \n",
        "    neural_network = NeuralNet() \n",
        "  \n",
        "    # The training set. \n",
        "    inputs = array([[0, 1, 1], [1, 0, 0], [1, 0, 1]]) \n",
        "    outputs = array([[1, 0, 1]]).T \n",
        "  \n",
        "    # Train the neural network \n",
        "    neural_network.train(inputs, outputs, 10) \n",
        "  \n",
        "    # Test the neural network with a test example. \n",
        "    print (neural_network.learn(array([1, 1, 0])) )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.73438134]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiGXJd2mIwnO",
        "colab_type": "text"
      },
      "source": [
        "**Training Using Tanh as Activation Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLvUtEP3FrNu",
        "colab_type": "code",
        "outputId": "ade1a5e8-37ad-4639-aef1-2904b3a2f31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# Class to create a neural  \n",
        "# network with single neuron \n",
        "class NeuralNetwork(): \n",
        "      \n",
        "    def __init__(self): \n",
        "          \n",
        "        # Using seed to make sure it'll   \n",
        "        # generate same weights in every run \n",
        "        random.seed(1) \n",
        "          \n",
        "        # 3x1 Weight matrix \n",
        "        self.weight_matrix = np.random.randn(3, 1)\n",
        "  \n",
        "    # tanh as activation fucntion \n",
        "    def tanh(self, x):\n",
        "        return ((exp(x) - exp(-x))/(exp(x) + exp(-x)))\n",
        "  \n",
        "    # derivative of tanh function. \n",
        "    # Needed to calculate the gradients. \n",
        "    def tanh_derivative(self, x): \n",
        "        return (1 - tanh(x)**2)\n",
        "  \n",
        "    # forward propagation \n",
        "    def forward_propagation(self, inputs): \n",
        "        return self.tanh(dot(inputs, self.weight_matrix)) \n",
        "      \n",
        "    # training the neural network. \n",
        "    def train(self, train_inputs, train_outputs, \n",
        "                            num_train_iterations): \n",
        "                                  \n",
        "        # Number of iterations we want to \n",
        "        # perform for this set of input. \n",
        "        for iteration in range(num_train_iterations): \n",
        "            output = self.forward_propagation(train_inputs) \n",
        "  \n",
        "            # Calculate the error in the output. \n",
        "            error = train_outputs - output \n",
        "  \n",
        "            # multiply the error by input and then  \n",
        "            # by gradient of tanh funtion to calculate \n",
        "            # the adjustment needs to be made in weights \n",
        "            adjustment = dot(train_inputs.T, error *\n",
        "                             self.tanh_derivative(output)) \n",
        "                               \n",
        "            # Adjust the weight matrix \n",
        "            self.weight_matrix += adjustment \n",
        "  \n",
        "# Driver Code \n",
        "if __name__ == \"__main__\": \n",
        "      \n",
        "    neural_network = NeuralNetwork() \n",
        "      \n",
        "    print ('Random weights at the start of training') \n",
        "    print (neural_network.weight_matrix) \n",
        "  \n",
        "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) \n",
        "    train_outputs = array([[0, 1, 1, 0]]).T \n",
        "  \n",
        "    neural_network.train(train_inputs, train_outputs, 10) \n",
        "  \n",
        "    print ('New weights after training') \n",
        "    print (neural_network.weight_matrix) \n",
        "  \n",
        "    # Test the neural network with a new situation. \n",
        "    print (\"Testing network on new examples ->\")\n",
        "    print (neural_network.forward_propagation(array([1, 0, 0])))\n",
        "     "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random weights at the start of training\n",
            "[[ 1.62434536]\n",
            " [-0.61175641]\n",
            " [-0.52817175]]\n",
            "New weights after training\n",
            "[[ 2.37300944]\n",
            " [-0.19439288]\n",
            " [-0.33720383]]\n",
            "Testing network on new examples ->\n",
            "[0.98277719]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g05CUjg3NpSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jpxV4bHEARR",
        "colab_type": "text"
      },
      "source": [
        "**Training Using ReLU as Activation Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfincNx5Dblg",
        "colab_type": "code",
        "outputId": "54290370-5a46-46df-904c-9683e94e2516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "# Class to create a neural  \n",
        "# network with single neuron \n",
        "class NeuralNetwork(): \n",
        "      \n",
        "    def __init__(self): \n",
        "          \n",
        "        # Using seed to make sure it'll   \n",
        "        # generate same weights in every run \n",
        "        random.seed(1) \n",
        "          \n",
        "        # 3x1 Weight matrix \n",
        "        self.weight_matrix = np.random.randn(3, 1)\n",
        "  \n",
        "    # tanh as activation fucntion \n",
        "    def ReLU(self, x):\n",
        "        return np.maximum(0,x)\n",
        "  \n",
        "    # derivative of tanh function. \n",
        "    # Needed to calculate the gradients. \n",
        "    def ReLU_derivative(self, x): \n",
        "        if x>0:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "  \n",
        "    # forward propagation \n",
        "    def forward_propagation(self, inputs): \n",
        "        return self.ReLU(dot(inputs, self.weight_matrix)) \n",
        "      \n",
        "    # training the neural network. \n",
        "    def train(self, train_inputs, train_outputs, \n",
        "                            num_train_iterations): \n",
        "                                  \n",
        "        # Number of iterations we want to \n",
        "        # perform for this set of input. \n",
        "        for iteration in range(num_train_iterations): \n",
        "            output = self.forward_propagation(train_inputs) \n",
        "  \n",
        "            # Calculate the error in the output. \n",
        "            error = train_outputs - output \n",
        "  \n",
        "            # multiply the error by input and then  \n",
        "            # by gradient of tanh funtion to calculate \n",
        "            # the adjustment needs to be made in weights \n",
        "            adjustment = dot(train_inputs.T, error *\n",
        "                             self.ReLU_derivative(output)) \n",
        "                               \n",
        "            # Adjust the weight matrix \n",
        "            self.weight_matrix += adjustment \n",
        "  \n",
        "# Driver Code \n",
        "if __name__ == \"__main__\": \n",
        "      \n",
        "    neural_network = NeuralNetwork() \n",
        "      \n",
        "    print ('Random weights at the start of training') \n",
        "    print (neural_network.weight_matrix) \n",
        "  \n",
        "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) \n",
        "    train_outputs = array([[0, 1, 1, 0]]).T \n",
        "  \n",
        "    neural_network.train(train_inputs, train_outputs, 10) \n",
        "  \n",
        "    print ('New weights after training') \n",
        "    print (neural_network.weight_matrix) \n",
        "  \n",
        "    # Test the neural network with a new situation. \n",
        "    print (\"Testing network on new examples ->\")\n",
        "    print (neural_network.forward_propagation(array([1, 0, 1])))\n",
        "     "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random weights at the start of training\n",
            "[[ 1.62434536]\n",
            " [-0.61175641]\n",
            " [-0.52817175]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-518611422599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtrain_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'New weights after training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-518611422599>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_inputs, train_outputs, num_train_iterations)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# the adjustment needs to be made in weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             adjustment = dot(train_inputs.T, error *\n\u001b[0;32m---> 44\u001b[0;31m                              self.ReLU_derivative(output)) \n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Adjust the weight matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-518611422599>\u001b[0m in \u001b[0;36mReLU_derivative\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Needed to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mReLU_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cHxDqmyGjEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}